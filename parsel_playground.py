"""
This program is a development snapshot of the `parsel_train.py` script.
It was successfully used to test new features of the forecast for one
date and location, hardcodedly selected.
"""

import argparse
import datetime
import os
from pathlib import Path

import numpy as np
import pandas as pd
import yaml

from rtrend_forecast.reporting import ExecTimeTracker, get_rtrend_logger
from rtrend_forecast.structs import RtData, ConstGammaTg
from rtrend_forecast.preprocessing import apply_lowpass_filter_pdseries
from rtrend_interface.forecast_operators import (
    WEEKLEN,
    ParSelTrainOperator,
    ParSelTrainOperatorLegacy,
)
from rtrend_interface.parsel_utils import load_population_data, make_date_state_index, load_truth_cdc_simple, make_mcmc_prefixes, \
    make_file_friendly_name, make_rt_out_fname, make_filtered_fname

_CALL_TIME = datetime.datetime.now()


# DEFAULT PARAMETERS
# -------------
DEFAULT_PARAMS = dict(  # Parameters that go in the main Params class
    # Field names in the preprocessed data file
    filt_col_name="filtered",
    filtround_col_name="filtered_round",

    # Parallelization
    ncpus_dates=1,
    ncpus_states=1,
)

DEFAULT_PARAMS_GENERAL = dict(  # Parameters from the `general` dict
    pop_data_path="population_data/locations.csv",
    forecast_days_ahead=2,  # Forecast evaluation ahead of the day_pres
)

# Helpful objects from the Rtrend library
LOCAL_XXT = ExecTimeTracker(category=__name__)
_LOGGER = get_rtrend_logger().getChild(__name__)


#


# -------------------------------------------------------------------
# MAIN FUNCTION
# -------------------------------------------------------------------

def main():

    args: CLArgs = parse_args()
    params: Params = import_params(args)
    data = Data()

    import_simulation_data(params, data)

    # run_training(params, data)  # THIS WILL RUN THE SEASON MULTIPLE TIMES
    # run_forecasts_once(params, data)  # RUNS FOR ONE SEASON!

    # --- TEST FUNCTION
    dev_synth_one_data(params, data)
#


#

# -------------------------------------------------------------------
# PROGRAM STRUCTURES
# -------------------------------------------------------------------


class CLArgs:
    """Command line arguments."""
    input_file: Path
    # output_dir: Path  # Not required here
    export: bool


class Params:
    """All parameters. Include hardcoded script params, those read
    from inputs, and overriden by command line flags.
    """
    input_dict: dict  # Contains all inputs read from the file.
    call_time: datetime.datetime

    general: dict
    preprocessing: dict
    rt_estimation: dict
    rt_synthesis: dict
    inc_reconstruction: dict
    postprocessing: dict

    # Program parameters
    filt_col_name: str
    filtround_col_name: str
    ncpus_dates: int
    ncpus_states: int


class Data:
    """Input, intermediate and output data.
    Input data is reserved for large structures. Smaller ones that
    can be manually set should be reserved as parameters.
    """
    pop_df: pd.DataFrame  # Data frame with population demographic data
    all_state_names: list  # Names of all available locations to forecast
    state_name_to_id: dict  # From `location_name` to `location`

    day_pres_seq: pd.DatetimeIndex  # Sequence of present days to use
    use_state_names: None  # Names of locations that shall be used.
    date_state_idx: pd.MultiIndex  # Combined index for dates and states
    # summary_metrics_df: pd.DataFrame  # Overall summary metrics


#


#


# -------------------------------------------------------------------
# PROGRAM PROCEDURES
# -------------------------------------------------------------------

def parse_args():
    """Interprets and stores the command line arguments."""

    parser = argparse.ArgumentParser(
        # usage="[[COMMAND LINE SIGNATURE (autogenerated)[]",
        description="Promotes the preprocessing and R(t) estimation "
                    "for the Rtrend FluH model. Stores results into "
                    "buffer files that can be used in the future.",
        # epilog="[[TEXT DISPLAYED AFTER ARGUMENTS DESCRIPTION]]",
    )

    # --- Positional paths
    parser.add_argument(
        "input_file", type=Path,
        help="Path to the file with input parameters."
    )
    parser.add_argument(
        "output_dir", type=Path,
        help="Path to the output directory, where all "
             "output files are stored."
    )

    #
    # --- Optional flags - SET TO NONE to have no effect
    parser.add_argument(
        "--export", action=argparse.BooleanOptionalAction,
        help="Whether the outputs (results of the preprocessing and "
             "R(t) estimation) should be exported to files.",
        default=None,
    )

    return parser.parse_args()  # When all arguments are defined here
    # return parser.parse_known_args()  # If there are extra arguments


def import_params(args: CLArgs):
    """Imports the main YAML parameter file."""

    # --- Read file
    with open(args.input_file, "r") as fp:
        input_dict = yaml.load(fp, yaml.Loader)

    # --- Populate the Params in priority order
    # Script Default < Input File < Command Line Arguments
    params = Params()
    params.call_time = _CALL_TIME  # Datetime of program call

    params.__dict__.update(DEFAULT_PARAMS)
    params.__dict__.update(input_dict)
    params.__dict__.update(
        {key: val for key, val in args.__dict__.items()
         if val is not None})

    # params.input_dict = input_dict  # Use this to keep the input_dict

    # --- Inner parameters overriding
    for key, val in DEFAULT_PARAMS_GENERAL.items():
        if key not in params.general:
            params.general[key] = val

    # You can rename parameters here.

    return params


def import_simulation_data(params: Params, data: Data):
    # --------------------
    # IMPORT DEMOGRAPHIC DATA
    # --------------------
    try:
        pop_fname = params.general["pop_data_path"]
    except KeyError:
        raise KeyError(
            "Hey, parameter 'general.pop_data_path' is required "
            "(must inform the path to the population file).")

    data.__dict__.update(load_population_data(pop_fname))

    # -------------------
    # IMPORT OR PREPARE PREPROCESSING DATA
    # -------------------

    # THIS May be done with a stream instead. Let's see.


#


#


#

#


# -------------------------------------------------------------------
# AUXILIARY PROCEDURES
# -------------------------------------------------------------------


#


# -------------------------------------------------------------------
# MAIN TRAINING ROUTINES
# -------------------------------------------------------------------


#


def run_forecasts_once(params: Params, data: Data):
    """This main-level function runs the forecast with the
    given parameters, for all locations and selected dates.

    It implements the "Forecast" pipeline.
    """
    raise NotImplementedError


def dev_synth_one_data(params: Params, data: Data):
    """"""
    # --------------------
    # Prepare the loop
    # --------------------
    data.day_pres_seq, data.use_state_names, data.date_state_idx = (
        make_date_state_index(params.general, data.all_state_names))

    main_roi_len = (
        pd.Timedelta(params.general["nperiods_main_roi"], "w"))

    use_state_names = data.use_state_names
    state_name_to_id = data.state_name_to_id
    day_pres_seq = data.day_pres_seq

    # -----------
    #  ######################## ###  # # # # ########
    # FAKE LOOp over date and state TODO
    # day_pres, state_name = data.date_state_idx[0] # ### # # # #
    day_pres = day_pres_seq[4]

    # state_name = use_state_names[0]  # By INDEX
    state_name = "Illinois"  # "California"  # "Nebraska"  # "Wisconsin" # "Wyoming" # "North Dakota"  # "California" # Indiana # Illinois
    _sn = make_file_friendly_name(state_name)

    day_forecast = (
            day_pres
            + pd.Timedelta(params.general["forecast_days_ahead"], "d"))

    day_pres_str = day_pres.date().isoformat()
    day_forecast_str = day_forecast.date().isoformat()

    # filt_fname = make_filtered_fname(
    #     date_str, state_str, params.general["preproc_data_dir"])

    # Truth data loading
    # ------------------
    truth_fname = os.path.join(
        params.general["hosp_data_dir"],
        params.general["hosp_data_fmt"].format(date=day_forecast_str))
    # hosp: CDCDataBunch = load_cdc_truth_data(truth_fname)
    # truth_df = hosp.df

    truth_df = load_truth_cdc_simple(truth_fname)  # About 5MB.
    #           ^ ^ Passed to subprocess.
    _LOGGER.info(f"Pop file {truth_fname} loaded.")

    # WATCH â€“ LOOKS LIKE I WAS USING THE WRONG FILES!!
    print(f"DAY PRES = {day_pres_str}")
    print(truth_df.xs(state_name_to_id[state_name], level="location"))


    # ROI definition (main ROI, that's passed to the forecast)
    # --------------
    roi_start = day_pres - main_roi_len
    roi_end = day_pres

    # Create generation time object
    tg = ConstGammaTg(
        params.general["tg_shape"],
        params.general["tg_rate"],
        tmax=params.general["tg_max"]
    )

    # State truth series
    state_series = truth_df.xs(
        state_name_to_id[state_name], level="location")["value"]

    # Create forecast operator
    # ------------------------
    fop = ParSelTrainOperatorLegacy(
        roi_start, roi_end,
        state_series,  # Full state series
        params=params.__dict__,
        nperiods_fore=WEEKLEN * params.general["nperiods_fore"],
        name=f"{_sn}_{day_pres.date().isoformat()}",
        tg_past=tg,
    )

    fop.run()

    # ---------
    # #################
    #  # # #          ### #_ #_ ## # #_#_ #_
    # TODO  -------------- HERE STARTS THE SANDBOX ---------------
    # Preparing the R(t) data
    # print(make_mcmc_prefixes(
    #     day_pres_str, _sn, params.general["preproc_data_dir"]))
    rt_fname = make_rt_out_fname(
        day_pres_str, _sn,
        params.general["preproc_data_dir"])

    # ---------- LOAD R(t) DATA FINALLY ---------------
    rt = RtData(np.loadtxt(rt_fname))
    preproc_df = pd.read_csv(
        make_filtered_fname(day_pres_str, _sn,
                            params.general["preproc_data_dir"]),
        index_col=0, parse_dates=[0],
    )
    ct_past = preproc_df["filtered"]  # NOT SCALED! I'm using just filtered instead of filtered_round!

    # DETERMINE THE DATE RANGE OF THE LOADED MCMC
    # TODO: that's better made by just taking the length of the mcmc signal
    # For now, "hardcoded" like this:
    mcmc_roi_len = params.rt_estimation["roi_len"]
    rt_last_day = day_pres - pd.Timedelta("1d")
    rt_first_day = day_pres - pd.Timedelta(mcmc_roi_len, "d")
    rt_dates = pd.date_range(rt_first_day, rt_last_day, freq="d")

    # Assign dates
    rt.df.columns = rt_dates

    #

    #

    #

    # -------------- CALCULATE R(T) STATS ------------
    from rtrend_forecast.rt_synthesis import get_sorted_mean_ensemble

    # Average ensemble
    mean_vals = get_sorted_mean_ensemble(
        rt, **{key: params.rt_synthesis[key]
               for key in ["q_low", "q_hig", "nperiods_past", "r_max"]}
    )

    # Number of samples from the sorted ensemble
    nsamples = mean_vals.shape[0]

    # Median time series
    median = rt.get_median()

    # ---------------- PREPROCESSING DATA LOADING -------------------------
    preproc_summary_path = os.path.join(params.general["preproc_data_dir"], "preproc_summary.csv")
    preproc_summary_df = pd.read_csv(preproc_summary_path, index_col=[0, 1], parse_dates=[0])

    cumhosp_season = preproc_summary_df.loc[(day_pres, state_name)]["cumhosp_season_filt"]

    # ---------------- SYNTHESIZE RT FORE (with drift method) + RECONSTRUCT NEXT STEPS ----------------
    # print()
    ndays_fore = WEEKLEN * params.general["nperiods_fore"]

    # Allocate and initialize arrays of forecasted trajectories
    ct_fore_2da = np.zeros((fop.nperiods_fore_gran, nsamples))
    rt_fore_2da = np.zeros((fop.nperiods_fore_gran, nsamples))
    #  ^  ^  Signature for both: a[i_period, i_sample]

    # Seeds synthesis random number generator
    _rng = np.random.default_rng(params.rt_synthesis["seed"])

    # # FIRST STEP IN R(T) FORE CALCULATION (requires data from last day, ct[-1]
    rt_next = _calc_rt_next_tmp(mean_vals, nsamples, ct_past[-1], _rng)

    for i_t_fore in range(ndays_fore):
        # Synthesis
        # ---------
        # C(T) calculation
        # -- Stores the next R(t) (calculated in previous step)
        rt_fore_2da[i_t_fore, :] = rt_next

        # Reconstruction
        # --------------
        tg_array = tg.get_pmf_array()
        step_reconstruction(
            i_t_fore, ct_past.values, rt_fore_2da, tg_array, tg_array,
            tg.tmax, ct_fore_2da, nsamples,
            params.inc_reconstruction["seed"]
        )

        # # Calculates the next R(t)
        rt_next = _calc_rt_next_tmp(rt_fore_2da[i_t_fore], nsamples, ct_fore_2da[i_t_fore], _rng)

    # ----------------

    # # WATCH
    print(mean_vals)
    # print(r_start)
    # print(rt_fore_2da)
    # print(rt_fore.array)
    # print(ct_past)
    print(rt_next)
    # print(ct_fore_2da)

    # WATCH-PLOT
    import matplotlib as mpl
    import matplotlib.pyplot as plt
    from rtrend_forecast.utils import rotate_ax_labels

    fig, axes = plt.subplots(nrows=2, figsize=(10, 6))

    # R(t) plots
    # ----------
    ax = axes[0]

    # --- PAST R(t)
    past_idx = fop.time.past_gran_idx[-rt.nperiods:]
    ax.plot(past_idx, rt.get_median())
    ax.fill_between(
        past_idx,
        *rt.get_quantiles(),
        alpha=0.25
    )
    ax.plot([fop.time.pg0, fop.time.fg1], [1, 1], "k--", alpha=0.5)

    # --- FORECASTED R(t)
    ax.plot(fop.time.fore_gran_idx, np.median(rt_fore_2da, axis=1))
    ax.fill_between(
        fop.time.fore_gran_idx,
        np.quantile(rt_fore_2da, q=0.025, axis=1),
        np.quantile(rt_fore_2da, q=0.975, axis=1),
        alpha=0.25
    )

    # SET

    ax.text(0.8, 0.9, state_name, transform=ax.transAxes)
    ax.text(0.8, 0.8, day_pres_str, transform=ax.transAxes)

    rotate_ax_labels(ax)

    # C(t) plots
    # ----------
    ax = axes[1]
    # scale_fac = fop.preproc_params["scale_factor"]
    scale_fac = 1.  # fop.preproc_params["scale_factor"]  # BYPASS SCALE FACTOR

    # Past c(t)
    ax.plot(fop.inc.past_gran_sr)

    # Filtered past c(t)
    ax.plot(apply_lowpass_filter_pdseries(fop.inc.past_gran_sr, cutoff=0.1))

    # # C(t) forecast
    ax.plot(fop.time.fore_gran_idx, np.median(ct_fore_2da, axis=1) / scale_fac, "--")
    ax.fill_between(
        fop.time.fore_gran_idx,
        np.quantile(ct_fore_2da, q=0.025, axis=1) / scale_fac,
        np.quantile(ct_fore_2da, q=0.975, axis=1) / scale_fac,
        alpha=0.25,
    )
    ax.fill_between(
        fop.time.fore_gran_idx,
        np.quantile(ct_fore_2da, q=0.25, axis=1) / scale_fac,
        np.quantile(ct_fore_2da, q=0.75, axis=1) / scale_fac,
        alpha=0.25,
    )

    # --- Future C(t) (read from latest file)
    latest_truth_df = load_truth_cdc_simple("hosp_data/daily_truth-Incident Hospitalizations.csv")
    fore_series = latest_truth_df.loc[fop.time.fore_gran_idx].xs(data.state_name_to_id[state_name], level="location")["value"]

    ax.plot(fore_series)

    rotate_ax_labels(ax)

    # --- General axes setup
    import matplotlib.dates
    for ax in axes:
        ax.xaxis.set_major_locator(mpl.dates.WeekdayLocator())
        ax.set_xlim(fop.time.pg1 - 40 * fop.gran_dt, fop.time.fg1 + 2 * fop.gran_dt)

    fig.tight_layout()
    plt.show()


# ----------------
# FUTURE FUNCTIONS Zâ€“ These will be moved to the Rtrend package
import numba as nb


def _calc_rt_next_tmp(last_r_vals, nsamples, last_ct, _rng):
    """HARDCODED FUNCTION TO TEST MULTIPLE METHODS OF R(T) SYNTHESIS"""
    # --- INCIDENCE EXPONENTIAL DRIFT
    # TMP: define here drift parameters
    sigma = 0.00  # Width of the random walk normal steps

    # alpha = 2.E-5  # California. Weight of the current incidence to deplete susceptibles.
    alpha = 6.E-5  # Illinois. Weight of the current incidence to deplete susceptibles.
    # alpha = 10.E-4  # Wyoming. Weight of the current incidence to deplete susceptibles.

    bias = 0.000

    return last_r_vals * np.exp(
        sigma * _rng.normal(scale=sigma, size=nsamples)  # RW
        - alpha * last_ct + bias
    )


@nb.njit
def step_reconstruction(
        i_t_fore, ct_past, rt_fore_2d, tg_past, tg_fore, tg_max,
        ct_fore_2d, nsamples, seed
):
    """One hardcoded step of the reconstruction (renewal eq.)
    """
    np.random.seed(seed + i_t_fore)  # Seeds the numba or numpy generator

    # Main loop over R(t) samples
    for i_sample in range(nsamples):
        rt_fore = rt_fore_2d[:, i_sample]
        ct_fore = ct_fore_2d[:, i_sample]

        lamb = 0.  # Sum of generated cases from past cases
        r_curr = rt_fore[i_t_fore]

        # Future series chunk
        for st in range(1, min(i_t_fore + 1, tg_max)):
            lamb += r_curr * tg_fore[st] * ct_fore[i_t_fore - st]

        # Past series chunk
        for st in range(i_t_fore + 1, tg_max):
            lamb += r_curr * tg_past[st] * ct_past[-(st - i_t_fore)]

        # Poisson number
        ct_fore[i_t_fore] = np.random.poisson(lamb)


if __name__ == "__main__":
    main()


#


# =============================================================================
# ==================  OLD CODE VAULT  =========================================
# =============================================================================


# # -()- HARDCODED DYNAMIC RAMP
# r1, r2 = [params.rt_synthesis[k] for k in ["r1_start", "r2_start"]]
# r_start = (r2 - r1) * mean_vals + 2 * r1 - r2
# r1, r2 = [params.rt_synthesis[k] for k in ["r1_end", "r2_end"]]
# r_end = (r2 - r1) * mean_vals + 2 * r1 - r2
#
# rt_fore = RtData(np.linspace(r_start, r_end, ndays_fore).T)
#
# # -//-