"""
This script promotes pre-forecast steps using an opreator from the
Rtrend library. It promotes preprocessing and MCMC R(t) estimation
for selected locations and dates, with the main goal to buffer this
into files.

Most auxiliary definitions are taken from the `rtrend_interface`
directory.
"""

import argparse
import datetime
import os
from pathlib import Path

import pandas as pd
import yaml

from rtrend_forecast.reporting import ExecTimeTracker, get_rtrend_logger
from rtrend_forecast.structs import ConstGammaTg
from rtrend_forecast.utils import make_dir_from, \
    map_parallel_or_sequential

from utils.forecast_operators_flusight_2023 import \
    ParSelPreprocessOperator, WEEKLEN
from utils.parsel_utils import make_date_state_index, \
    load_truth_cdc_simple, make_file_friendly_name, \
    make_mcmc_prefixes, make_filtered_fname, \
    prepare_dict_for_yaml_export, load_population_data

# from rtrend_tools.utils import map_parallel_or_sequential, make_dir_from


_CALL_TIME = datetime.datetime.now()

# DEFAULT PARAMETERS
# -------------
DEFAULT_PARAMS = dict(  # Parameters that go in the main Params class
    # # Field names in the preprocessed data file (DEPRECATED?)
    # filt_col_name="filtered",
    # filtround_col_name="filtered_round",
    # scaled_col_name="scaled",

    # Parallelization (use either, not both)
    ncpus_dates=5,
    ncpus_states=1,
)

DEFAULT_PARAMS_GENERAL = dict(  # Parameters from the `general` dict
    pop_data_path="population_data/locations.csv",
    forecast_days_ahead=2,  # Forecast evaluation ahead of the day_pres
)

DEFAULT_PARAMS_PREPROCESSING = dict(
    # scale_factor=1.0,  # EXPERIMENTAL data scaling
)

# Helpful objects from the Rtrend library
LOCAL_XXT = ExecTimeTracker(category=__name__)
_LOGGER = get_rtrend_logger().getChild(__name__)

#

# -------------------------------------------------------------------
# MAIN FUNCTION
# -------------------------------------------------------------------


def main():

    args: CLArgs = parse_args()
    params: Params = import_params(args)
    data = Data()

    import_all(params, data)  # Apparently nothing
    run_preproc(params, data)
    export_summary(params, data)

    # Manually reporting exec time
    print("-------------------\nEXECUTION TIMES")
    for key, val in LOCAL_XXT.get_dict().items():
        print(key.ljust(25, ".") + f" {val:0.4f}s")


#

#


# -------------------------------------------------------------------
# PROGRAM STRUCTURES
# -------------------------------------------------------------------

class CLArgs:
    """Command line arguments."""
    input_file: Path
    # output_dir: Path  # Not required here
    export: bool


class Params:
    """All parameters. Include hardcoded script params, those read
    from inputs, and overriden by command line flags.

    Parameters will have type hints in PyCharm, but raise
    AttributeError if referenced before assignment.

    This class works more as an autocomplete guide. It will receive
    any parameter that is either defined in DEFAULT_PARAMS, the input
    file and command line arguments.
    """
    # Inherited from command line arguments
    input_file: Path
    export: bool

    input_dict: dict  # Contains all inputs read from the file.
    call_time: datetime.datetime

    general: dict
    preprocessing: dict
    rt_estimation: dict

    # Program parameters
    filt_col_name: str
    filtround_col_name: str
    scaled_col_name: str
    ncpus_dates: int
    ncpus_states: int


class Data:
    """Input, intermediate and output data.
    Input data is reserved for large structures. Smaller ones that
    can be manually set should be reserved as parameters.
    """
    pop_df: pd.DataFrame  # Data frame with population demographic data
    all_state_names: list  # Names of all available locations to forecast
    state_name_to_id: dict  # From `location_name` to `location`

    day_pres_seq: pd.DatetimeIndex  # Sequence of present days to use
    use_state_names: None  # Names of locations that shall be used.
    date_state_idx: pd.MultiIndex  # Combined index for dates and states
    summary_metrics_df: pd.DataFrame  # Overall summary metrics


#


#


# -------------------------------------------------------------------
# PROGRAM PROCEDURES
# -------------------------------------------------------------------

def parse_args():
    """Interprets and stores the command line arguments."""

    parser = argparse.ArgumentParser(
        # usage="[[COMMAND LINE SIGNATURE (autogenerated)[]",
        description="Promotes the preprocessing and R(t) estimation "
                    "for the Rtrend FluH model. Stores results into "
                    "buffer files that can be used in the future.",
        # epilog="[[TEXT DISPLAYED AFTER ARGUMENTS DESCRIPTION]]",
    )

    # --- Positional paths
    parser.add_argument(
        "input_file", type=Path,
        help="Path to the file with input parameters."
    )

    # parser.add_argument(
    #     "output_dir", type=Path,
    #     help="Path to the output directory, where all "
    #          "output files are stored."
    # )

    #
    # --- Optional flags - SET TO NONE to have no effect
    parser.add_argument(
        "--export", action=argparse.BooleanOptionalAction,
        help="Whether the outputs (results of the preprocessing and "
             "R(t) estimation) should be exported to files.",
        default=True,
    )

    return parser.parse_args()  # When all arguments are defined here
    # return parser.parse_known_args()  # If there are extra arguments


def import_params(args: CLArgs):
    """Imports the main YAML parameter file."""

    # --- Read file
    with open(args.input_file, "r") as fp:
        input_dict = yaml.load(fp, yaml.Loader)

    # --- Populate the Params in priority order
    # Script Default < Input File < Command Line Arguments
    params = Params()
    params.call_time = _CALL_TIME  # Datetime of program call

    params.__dict__.update(DEFAULT_PARAMS)
    params.__dict__.update(input_dict)
    params.__dict__.update(
        {key: val for key, val in args.__dict__.items()
         if val is not None})

    # params.input_dict = input_dict  # Use this to keep the input_dict

    # --- Inner parameters overriding
    for key, val in DEFAULT_PARAMS_GENERAL.items():
        if key not in params.general:
            params.general[key] = val

    for key, val in DEFAULT_PARAMS_PREPROCESSING.items():
        if key not in params.preprocessing:
            params.preprocessing[key] = val

    # You can rename parameters here.

    # ---
    if not params.export:
        _LOGGER.warning(
            "----------- RUNNING WITH EXPORT = FALSE -----------")

    return params


#

#


# -------------------------------------------------------------------
# AUXILIARY PROCEDURES
# -------------------------------------------------------------------

def import_all(params: Params, data: Data):
    # --------------------
    # IMPORT DEMOGRAPHIC DATA
    # --------------------
    try:
        pop_fname = params.general["pop_data_path"]
    except KeyError:
        raise KeyError(
            "Hey, parameter 'general.pop_data_path' is required "
            "(must inform the path to the population file).")

    data.__dict__.update(load_population_data(pop_fname))
    _LOGGER.info("Population file loaded.")


def calc_fop_postprocessing(
        fop: ParSelPreprocessOperator, params: Params,
        state_series: pd.Series
):
    """Calculate summary metrics from each preprocessing run."""
    out_dict = dict()
    season_start = params.general["season_start"]

    # --- Cumulative number of hospitalizations since `season_start`
    out_dict["cumhosp_season"] = (
        state_series.loc[season_start:].sum())
    # --- Cumulative number from the filtered data
    out_dict["cumhosp_season_filt"] = (
        fop.extra["past_denoised_sr"].loc[season_start:].sum()
    )
    # --- Data scaling factor for MCMC
    out_dict["scale_factor"] = fop.preproc_params.get("scale_factor", 1.0)

    return out_dict


def export_fop_data(
        fop: ParSelPreprocessOperator, params: Params,
        date_str, state_str):
    """Exports relevant information from a single forecast operator.
    """
    # Do not export if required so
    if not params.export:
        return

    # Export MCMC â€“ performed during preprocessing by the C engine
    # ------------
    # - THIS IS already done.

    # Export preprocessed temporal data
    # --------------------
    prep_fname = make_filtered_fname(
        date_str, state_str, params.general["preproc_data_dir"])
    make_dir_from(prep_fname)

    # Selects stored time series to export
    out_df = pd.DataFrame(
        {name: fop.extra[name] for name in
         ["past_denoised_sr", "past_scaled_int_sr"]}
    )

    # Export command
    out_df.to_csv(prep_fname)
    fop.logger.info(f"Exported filtered data to {prep_fname}")


def export_summary(
        params: Params, data: Data):

    # Do not export if required so
    if not params.export:
        return

    # Export parameters
    # --------------------
    params_fname = os.path.join(
        params.general["preproc_data_dir"],
        "params.yaml")

    p_dict: dict = params.__dict__.copy()  # Shallow copy
    prepare_dict_for_yaml_export(p_dict)
    make_dir_from(params_fname)

    with open(params_fname, "w") as fp:
        yaml.dump(p_dict, default_flow_style=False, stream=fp)

    # Export summary metrics
    # -----------------------
    summary_fname = os.path.join(
        params.general["preproc_data_dir"],
        "preproc_summary.csv")
    data.summary_metrics_df.to_csv(summary_fname)


#


#


# -------------------------------------------------------------------
# MAIN PREPROCESSING PROCEDURE
# -------------------------------------------------------------------


@LOCAL_XXT.track()
def run_preproc(params: Params, data: Data):
    """Having the parameters set up, runs the preprocessing routine
    for all the data points.
    """

    # --------------------
    # Prepare the loop
    # --------------------
    data.day_pres_seq, data.use_state_names, data.date_state_idx = (
        make_date_state_index(params.general, data.all_state_names))

    main_roi_len = (
        pd.Timedelta(params.general["nperiods_main_roi"], "w"))

    # --- Unpack structs to avoid copies of the entire Data in processes
    use_state_names = data.use_state_names
    state_name_to_id = data.state_name_to_id
    day_pres_seq = data.day_pres_seq
    pop_df = data.pop_df

    # --------------------
    # Loop over dates (as day_pres)
    # --------------------
    def date_task(inputs):
        """Do all the processing for one date.
        """

        # TODO: THE ENTIRE PREPWORK FOR THIS IS QUITE COMMOn for other pipelines
        #   General methods?

        i_date, day_pres = inputs
        day_pres: pd.Timestamp
        day_forecast = (
                day_pres
                + pd.Timedelta(params.general["forecast_days_ahead"], "d"))

        day_pres_str = day_pres.date().isoformat()
        day_forecast_str = day_forecast.date().isoformat()

        # Truth data loading
        # ------------------
        truth_fname = os.path.join(
            params.general["hosp_data_dir"],
            params.general["hosp_data_fmt"].format(date=day_forecast_str))
        # hosp: CDCDataBunch = load_cdc_truth_data(truth_fname)
        # truth_df = hosp.df

        truth_df = load_truth_cdc_simple(truth_fname)  # About 5MB.
        #           ^ ^ Passed to subprocess.
        _LOGGER.info(f"Truth file {truth_fname} loaded.")

        # ROI definition (main ROI, that's passed to the forecast)
        # --------------
        roi_start = day_pres - main_roi_len
        roi_end = day_pres

        #

        # --------------------
        # Loop over location (states)
        # --------------------
        def state_task(state_inputs):
            """Performs the preprocessing pipeline for one location."""
            i_state, state_name = state_inputs
            _sn = make_file_friendly_name(state_name)

            # Add population size
            params.preprocessing["pop_size"] = pop_df.loc[state_name]["population"]

            # Create generation time object
            tg = ConstGammaTg(
                params.general["tg_shape"],
                params.general["tg_rate"],
                tmax=params.general["tg_max"],
            )

            # State truth series
            state_series = truth_df.xs(
                state_name_to_id[state_name], level="location")["value"]

            # Create forecast operator
            # ------------------------
            fop = ParSelPreprocessOperator(
                roi_start, roi_end,
                state_series,  # Full state series
                _sn=_sn, day_pres_str=day_pres_str,
                params=params.__dict__,
                nperiods_fore=WEEKLEN * params.general["nperiods_fore"],
                name=f"{_sn}_{day_pres.date().isoformat()}",
                tg_past=tg,
            )

            # Run the forecast preprocessing pipeline
            # ---------------------------------------
            fop.run()

            # --------------------
            # Export the relevant data
            # --------------------
            export_fop_data(fop, params, day_pres_str, _sn)
            post_dict = calc_fop_postprocessing(fop, params, state_series)

            # ---
            fop.dump_heavy_stuff()

            return post_dict  # Dictionary of postprocess metrics

        # -------------------------
        # -------------------------

        state_contents = list(enumerate(use_state_names))

        state_results = map_parallel_or_sequential(
            state_task, state_contents, ncpus=params.ncpus_states)
        # Results: list of dictionaries with summary metrics, one for each state

        out_df = pd.DataFrame(state_results, index=use_state_names)
        out_df.index.name = "state"
        return out_df

    # ---
    date_contents = enumerate(data.day_pres_seq)
    date_results = map_parallel_or_sequential(
        date_task, date_contents, ncpus=params.ncpus_dates)

    # -------------------#------------------
    # Final concatenation of summary metrics
    # -------------------#------------------
    data.summary_metrics_df = pd.concat(
        date_results, axis=0, keys=day_pres_seq, names=("day_pres",)
    )


if __name__ == "__main__":
    main()
